"""
Web Interface Module

This module provides a simplified web interface for the RAG assistant using Streamlit.
åªä¿ç•™ï¼šé€‰æ‹©collectionã€é€‰æ‹©LLMæ¨¡å‹ã€è®¾ç½®top_kå’Œå¯¹è¯åŠŸèƒ½
"""

import os
import streamlit as st
from dotenv import load_dotenv
from typing import List, Optional
import chromadb
from collection_metadata import get_embedding_model
from rag_pipeline import RAGPipeline

# Try to load API key from various sources
def get_api_key() -> Optional[str]:
    """
    Try to get API key from various sources
    
    Returns:
        API key or None
    """
    # Try from environment variables
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        return api_key
    
    # Try to load from .env file
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        return api_key
    
    # Try to read directly from .env file
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        env_path = os.path.join(current_dir, '.env')
        if os.path.isfile(env_path):
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.startswith('OPENAI_API_KEY'):
                        api_key = line.split('=', 1)[1].strip().strip('"').strip("'")
                        # Manually set environment variable
                        os.environ["OPENAI_API_KEY"] = api_key
                        return api_key
    except Exception as e:
        st.error(f"Error reading .env file directly: {e}")
    
    return None

# Load API key
api_key = get_api_key()

# Session state initialization
if "rag_pipeline" not in st.session_state:
    st.session_state.rag_pipeline = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "collection_name" not in st.session_state:
    st.session_state.collection_name = "documents"
if "persist_dir" not in st.session_state:
    st.session_state.persist_dir = "chroma_db"
if "current_model" not in st.session_state:
    st.session_state.current_model = "gpt-4o-mini"
if "prompt_template" not in st.session_state:
    st.session_state.prompt_template = "balanced"

def get_collections(persist_dir: str) -> List[str]:
    """
    Get all existing collections in the specified directory
    
    Args:
        persist_dir: ChromaDB storage directory
    
    Returns:
        List[str]: Collection name list
    """
    try:
        if not os.path.exists(persist_dir):
            return []
            
        client = chromadb.PersistentClient(path=persist_dir)
        collections = client.list_collections()
        return [col.name for col in collections]
    except Exception as e:
        st.warning(f"Unable to get existing collections: {e}")
        return []

def get_collection_info(persist_dir: str, collection_name: str) -> dict:
    """
    Get information about the specified collection
    
    Args:
        persist_dir: ChromaDB storage directory
        collection_name: Collection name
        
    Returns:
        dict: Collection information, including document count
    """
    try:
        if not os.path.exists(persist_dir):
            return {"exists": False, "count": 0}
            
        client = chromadb.PersistentClient(path=persist_dir)
        collections = client.list_collections()
        collection_names = [col.name for col in collections]
        
        if collection_name in collection_names:
            collection = client.get_collection(collection_name)
            count = collection.count()
            return {"exists": True, "count": count}
        return {"exists": False, "count": 0}
    except Exception as e:
        st.warning(f"Unable to get collection info: {e}")
        return {"exists": False, "count": 0, "error": str(e)}

def initialize_pipeline(api_key: str, llm_model: str, top_k: int, collection_name: str, prompt_template: str = "balanced") -> bool:
    """
    Initialize the RAG pipeline.
    
    Args:
        api_key: OpenAI API key
        llm_model: LLM model to use for answer generation
        top_k: Number of documents to retrieve
        collection_name: Name of the collection to use
        prompt_template: Prompt template to use (precise, balanced, creative)
    """
    try:
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œè·å–æ­£ç¡®çš„åµŒå…¥æ¨¡å‹
        embedding_model = get_embedding_model(collection_name)
        if not embedding_model:
            embedding_model = "sentence-transformers/all-MiniLM-L6-v2"  # é»˜è®¤å€¼
            st.info(f"Using default embedding model: {embedding_model}")
        else:
            st.info(f"Using embedding model from collection metadata: {embedding_model}")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ›´æ”¹
        model_changed = "current_model" in st.session_state and st.session_state.current_model != llm_model
        template_changed = "prompt_template" in st.session_state and st.session_state.prompt_template != prompt_template
        
        # ä¿å­˜å½“å‰æ¨¡æ¿
        st.session_state.prompt_template = prompt_template
        
        st.session_state.rag_pipeline = RAGPipeline(
            embedding_model=embedding_model,
            llm_model=llm_model,
            top_k=top_k,
            api_key=api_key,
            persist_dir=st.session_state.persist_dir,
            collection_name=collection_name,
            prompt_template=prompt_template
        )
        
        # ä¿å­˜å½“å‰æ¨¡å‹
        st.session_state.current_model = llm_model
        
        # è·å–æ¨¡å‹ä»‹ç»
        model_intro = st.session_state.rag_pipeline.get_model_introduction()
        template_info = st.session_state.rag_pipeline.get_current_template_info()
        
        # æ ¹æ®æ¨¡å‹å’Œæ¨¡æ¿æ˜¯å¦æ›´æ”¹æ˜¾ç¤ºä¸åŒæ¶ˆæ¯
        message = ""
        if model_changed:
            message += f"Model switched to {llm_model}. "
        if template_changed:
            message += f"Template switched to {template_info['name']}. "
        
        if message:
            st.session_state.model_message = message + model_intro
        else:
            st.session_state.model_message = f"Pipeline initialized with {llm_model} and {template_info['name']} template. {model_intro}"
        
        return True
    except Exception as e:
        st.error(f"Error initializing pipeline: {e}")
        return False

def main():
    """Main entry point for the Streamlit app."""
    st.set_page_config(
        page_title="Local Knowledge Base RAG Assistant",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸ¤– Local Knowledge Base RAG Assistant")
    
    # Sidebar for configuration
    with st.sidebar:
        st.header("Configuration")
        
        # If API key exists, display partial content
        if api_key:
            default_api_key = api_key
            api_key_help = f"Loaded from environment or .env file (starts with {api_key[:5]}...)"
        else:
            default_api_key = ""
            api_key_help = "Please enter your OpenAI API key"
            
        input_api_key = st.text_input("OpenAI API Key", 
                                      type="password", 
                                      value=default_api_key,
                                      help=api_key_help)
        
        # è·å–ç°æœ‰é›†åˆ
        collections = get_collections(st.session_state.persist_dir)
        
        # é›†åˆé€‰æ‹©åŒºåŸŸ
        st.subheader("Collection Settings")
        
        # é»˜è®¤é›†åˆåç§°
        default_collection = "documents"
        # å¦‚æœæœ‰ç°æœ‰é›†åˆï¼Œæä¾›é€‰æ‹©
        if collections:
            collection_options = collections
            default_index = 0 if default_collection not in collections else collections.index(default_collection)
            collection_name = st.selectbox(
                "Select Collection",
                options=collection_options,
                index=default_index
            )
            collection_info = get_collection_info(st.session_state.persist_dir, collection_name)
            if collection_info["exists"]:
                st.info(f"Collection contains {collection_info['count']} documents")
        else:
            collection_name = st.text_input("Collection Name", value=default_collection)
            st.info(f"No existing collections found. Please initialize a collection first via CLI.")
        
        # æ¨¡å‹é€‰æ‹©åŒºåŸŸ
        st.subheader("Model Settings")
        
        model_options = {
            "gpt-4o-mini": "GPT-4o Mini (å¿«é€Ÿ)",
            "gpt-3.5-turbo": "GPT-3.5 Turbo (å¿«é€Ÿ)",
            "gpt-4o": "GPT-4o (é«˜è´¨é‡)"
        }
        
        # æ¨¡å‹é€‰æ‹©
        default_model = "gpt-4o-mini"
        selected_model = st.selectbox(
            "Select LLM Model",
            options=list(model_options.keys()),
            format_func=lambda x: model_options.get(x, x),
            index=list(model_options.keys()).index(default_model)
        )
        
        # æ·»åŠ æç¤ºè¯æ¨¡æ¿é€‰æ‹©
        st.subheader("Prompt Template")
        template_options = {
            "precise": "Precise (ç²¾å‡†æ¨¡å¼)",
            "balanced": "Balanced (å¹³è¡¡æ¨¡å¼)",
            "creative": "Creative (åˆ›æ„æ¨¡å¼)"
        }
        
        template_descriptions = {
            "precise": "ä¸¥æ ¼éµå¾ªæ–‡æ¡£å†…å®¹ï¼Œæä¾›ç®€æ´å‡†ç¡®çš„å›ç­”",
            "balanced": "å¹³è¡¡å‡†ç¡®æ€§å’Œæµç•…æ€§ï¼Œé»˜è®¤æ¨¡å¼",
            "creative": "åœ¨ä¿æŒå‡†ç¡®çš„åŒæ—¶æä¾›æ›´è¯¦ç»†çš„è§£é‡Šå’Œè§è§£"
        }
        
        # è·å–å½“å‰æ¨¡æ¿
        current_template = st.session_state.get("prompt_template", "balanced")
        
        selected_template = st.selectbox(
            "Select Prompt Template",
            options=list(template_options.keys()),
            format_func=lambda x: template_options.get(x, x),
            index=list(template_options.keys()).index(current_template),
            help="é€‰æ‹©ä¸åŒçš„æç¤ºè¯æ¨¡æ¿æ¥æ§åˆ¶AIå›ç­”çš„é£æ ¼",
            key="template_selector"
        )
        
        # æ˜¾ç¤ºæ‰€é€‰æ¨¡æ¿çš„æè¿°
        st.caption(template_descriptions.get(selected_template, ""))
        
        # å¦‚æœå·²åˆå§‹åŒ–pipelineä¸”æ¨¡æ¿è¢«æ›´æ”¹ï¼Œæ›´æ–°æ¨¡æ¿
        if (st.session_state.rag_pipeline is not None and 
            selected_template != current_template and 
            "template_selector" in st.session_state):
            with st.spinner(f"Updating template to {template_options[selected_template]}..."):
                success = st.session_state.rag_pipeline.set_prompt_template(selected_template)
                if success:
                    st.session_state.prompt_template = selected_template
                    st.success(f"Template changed to {template_options[selected_template]}")
                    template_info = st.session_state.rag_pipeline.get_current_template_info()
                    st.session_state.model_message = f"Template switched to {template_info['name']}. {template_info['description']}"
                    st.rerun()
                else:
                    st.error("Failed to change template")
        
        # æ£€ç´¢å‚æ•°è®¾ç½®
        top_k = st.slider("Number of documents to retrieve (top_k)", min_value=1, max_value=20, value=5)
        
        # åˆå§‹åŒ–æŒ‰é’®
        if st.button("Initialize Pipeline", type="primary"):
            # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†é›†åˆ
            if not collection_name:
                st.error("Please select a collection first")
            else:
                # æ˜¾ç¤ºåŠ è½½æŒ‡ç¤ºå™¨
                with st.spinner("Initializing pipeline..."):
                    # å°è¯•åˆå§‹åŒ–pipeline
                    success = initialize_pipeline(
                        api_key=input_api_key or api_key,
                        llm_model=selected_model,
                        top_k=top_k,
                        collection_name=collection_name,
                        prompt_template=selected_template
                    )
                    if success:
                        st.success("Pipeline initialized successfully!")
                    else:
                        st.error("Failed to initialize pipeline. Check the error message above.")

    # å³ä¾§ä¸»åŒºåŸŸ
    if st.session_state.rag_pipeline is not None:
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        if "model_message" in st.session_state:
            st.info(st.session_state.model_message)
            
        # è·å–å½“å‰æ¨¡æ¿ä¿¡æ¯
        template_info = st.session_state.rag_pipeline.get_current_template_info()
        
        # æ˜¾ç¤ºèŠå¤©æ ‡é¢˜å’Œå½“å‰ä½¿ç”¨çš„æ¨¡æ¿ä¿¡æ¯
        st.subheader("Chat")
        st.caption(f"Using template: {template_info['name']} - {template_info['description']}")
        
        # æ ¹æ®æ¨¡æ¿è®¾ç½®å¤´åƒ
        avatar_emojis = {
            "precise": "ğŸ”",  # ç²¾ç¡®æ¨¡å¼ - æ”¾å¤§é•œ
            "balanced": "âš–ï¸",  # å¹³è¡¡æ¨¡å¼ - å¤©å¹³
            "creative": "ğŸ¨"   # åˆ›æ„æ¨¡å¼ - è°ƒè‰²æ¿
        }
        current_avatar_emoji = avatar_emojis.get(st.session_state.prompt_template, "âš–ï¸")
        
        # æ·»åŠ æ¨¡æ¿å›¾æ ‡è¯´æ˜
        st.caption("æ¨¡æ¿å›¾æ ‡: ğŸ”ç²¾ç¡®æ¨¡å¼ | âš–ï¸å¹³è¡¡æ¨¡å¼ | ğŸ¨åˆ›æ„æ¨¡å¼")
        
        # ä¿®å¤æ—§çš„å†å²è®°å½•ï¼Œç¡®ä¿æ¯æ¡æ¶ˆæ¯éƒ½æœ‰æ¨¡æ¿ä¿¡æ¯
        for message in st.session_state.chat_history:
            if message["role"] == "assistant" and "template" not in message:
                message["template"] = "balanced"  # ä¸ºæ—§æ¶ˆæ¯æ·»åŠ é»˜è®¤æ¨¡æ¿
        
        # æ˜¾ç¤ºèŠå¤©å†å²
        for message in st.session_state.chat_history:
            if message["role"] == "user":
                st.chat_message("user").write(message["content"])
            else:
                # ä½¿ç”¨æ¶ˆæ¯ä¸­ä¿å­˜çš„æ¨¡æ¿å¯¹åº”çš„å¤´åƒ
                message_template = message.get("template", "balanced")
                message_avatar = avatar_emojis.get(message_template, "âš–ï¸")
                st.chat_message("assistant", avatar=message_avatar).write(message["content"])
                
        # ç”¨æˆ·è¾“å…¥
        user_query = st.chat_input("Ask a question about your documents...")
        
        if user_query:
            # å°†ç”¨æˆ·é—®é¢˜æ·»åŠ åˆ°å†å²è®°å½•
            st.session_state.chat_history.append({"role": "user", "content": user_query})
            st.chat_message("user").write(user_query)
            
            # ç”Ÿæˆå›ç­”
            with st.chat_message("assistant", avatar=current_avatar_emoji):
                with st.spinner("Generating answer..."):
                    try:
                        answer = st.session_state.rag_pipeline.get_answer(user_query)
                        st.write(answer)
                        
                        # æ·»åŠ å›ç­”åˆ°å†å²è®°å½•ï¼ŒåŒ…æ‹¬å½“å‰ä½¿ç”¨çš„æ¨¡æ¿
                        st.session_state.chat_history.append({
                            "role": "assistant", 
                            "content": answer,
                            "template": st.session_state.prompt_template  # ä¿å­˜å½“å‰ä½¿ç”¨çš„æ¨¡æ¿
                        })
                    except Exception as e:
                        st.error(f"Error generating answer: {e}")
    else:
        st.info("Please initialize the pipeline in the sidebar to get started.")
        
        # æ·»åŠ ä¸€äº›ä½¿ç”¨æŒ‡å—
        with st.expander("Usage Guide", expanded=True):
            st.markdown("""
            ### Getting Started
            1. **Initialize Pipeline**: Select a Collection, Model and Template in the sidebar
            2. **Ask Questions**: Use the chat interface to query your knowledge base
            
            ### About Templates
            - **Precise**: Strictly follows document content with concise answers
            - **Balanced**: Balances accuracy and fluency (default)
            - **Creative**: Provides more detailed explanations while maintaining accuracy
            """)

if __name__ == "__main__":
    main() 