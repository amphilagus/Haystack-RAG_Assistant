"""
Web Interface Module

This module provides a simplified web interface for the RAG assistant using Streamlit.
åªä¿ç•™ï¼šé€‰æ‹©collectionã€é€‰æ‹©LLMæ¨¡å‹ã€è®¾ç½®top_kå’Œå¯¹è¯åŠŸèƒ½
"""

import os
import streamlit as st
from dotenv import load_dotenv
from typing import List, Optional
import chromadb
from collection_metadata import get_embedding_model
from rag_pipeline import RAGPipeline

# Try to load API key from various sources
def get_api_key() -> Optional[str]:
    """
    Try to get API key from various sources
    
    Returns:
        API key or None
    """
    # Try from environment variables
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        return api_key
    
    # Try to load from .env file
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        return api_key
    
    # Try to read directly from .env file
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        env_path = os.path.join(current_dir, '.env')
        if os.path.isfile(env_path):
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.startswith('OPENAI_API_KEY'):
                        api_key = line.split('=', 1)[1].strip().strip('"').strip("'")
                        # Manually set environment variable
                        os.environ["OPENAI_API_KEY"] = api_key
                        return api_key
    except Exception as e:
        st.error(f"Error reading .env file directly: {e}")
    
    return None

# Load API key
api_key = get_api_key()

# Session state initialization
if "rag_pipeline" not in st.session_state:
    st.session_state.rag_pipeline = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "collection_name" not in st.session_state:
    st.session_state.collection_name = "documents"
if "persist_dir" not in st.session_state:
    st.session_state.persist_dir = "chroma_db"
if "current_model" not in st.session_state:
    st.session_state.current_model = "gpt-4o-mini"

def get_collections(persist_dir: str) -> List[str]:
    """
    Get all existing collections in the specified directory
    
    Args:
        persist_dir: ChromaDB storage directory
    
    Returns:
        List[str]: Collection name list
    """
    try:
        if not os.path.exists(persist_dir):
            return []
            
        client = chromadb.PersistentClient(path=persist_dir)
        collections = client.list_collections()
        return [col.name for col in collections]
    except Exception as e:
        st.warning(f"Unable to get existing collections: {e}")
        return []

def get_collection_info(persist_dir: str, collection_name: str) -> dict:
    """
    Get information about the specified collection
    
    Args:
        persist_dir: ChromaDB storage directory
        collection_name: Collection name
        
    Returns:
        dict: Collection information, including document count
    """
    try:
        if not os.path.exists(persist_dir):
            return {"exists": False, "count": 0}
            
        client = chromadb.PersistentClient(path=persist_dir)
        collections = client.list_collections()
        collection_names = [col.name for col in collections]
        
        if collection_name in collection_names:
            collection = client.get_collection(collection_name)
            count = collection.count()
            return {"exists": True, "count": count}
        return {"exists": False, "count": 0}
    except Exception as e:
        st.warning(f"Unable to get collection info: {e}")
        return {"exists": False, "count": 0, "error": str(e)}

def initialize_pipeline(api_key: str, llm_model: str, top_k: int, collection_name: str) -> bool:
    """
    Initialize the RAG pipeline.
    
    Args:
        api_key: OpenAI API key
        llm_model: LLM model to use for answer generation
        top_k: Number of documents to retrieve
        collection_name: Name of the collection to use
    """
    try:
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œè·å–æ­£ç¡®çš„åµŒå…¥æ¨¡å‹
        embedding_model = get_embedding_model(collection_name)
        if not embedding_model:
            embedding_model = "sentence-transformers/all-MiniLM-L6-v2"  # é»˜è®¤å€¼
            st.info(f"Using default embedding model: {embedding_model}")
        else:
            st.info(f"Using embedding model from collection metadata: {embedding_model}")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ›´æ”¹
        model_changed = "current_model" in st.session_state and st.session_state.current_model != llm_model
        
        st.session_state.rag_pipeline = RAGPipeline(
            embedding_model=embedding_model,
            llm_model=llm_model,
            top_k=top_k,
            api_key=api_key,
            persist_dir=st.session_state.persist_dir,
            collection_name=collection_name
        )
        
        # ä¿å­˜å½“å‰æ¨¡å‹
        st.session_state.current_model = llm_model
        
        # è·å–æ¨¡å‹ä»‹ç»
        model_intro = st.session_state.rag_pipeline.get_model_introduction()
        
        # æ ¹æ®æ¨¡å‹æ˜¯å¦æ›´æ”¹æ˜¾ç¤ºä¸åŒæ¶ˆæ¯
        if model_changed:
            st.session_state.model_message = f"Model switched to {llm_model}. {model_intro}"
        else:
            st.session_state.model_message = f"Pipeline initialized with {llm_model}. {model_intro}"
        
        return True
    except Exception as e:
        st.error(f"Error initializing pipeline: {e}")
        return False

def main():
    """Main entry point for the Streamlit app."""
    st.set_page_config(
        page_title="Local Knowledge Base RAG Assistant",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸ¤– Local Knowledge Base RAG Assistant")
    
    # Sidebar for configuration
    with st.sidebar:
        st.header("Configuration")
        
        # If API key exists, display partial content
        if api_key:
            default_api_key = api_key
            api_key_help = f"Loaded from environment or .env file (starts with {api_key[:5]}...)"
        else:
            default_api_key = ""
            api_key_help = "Please enter your OpenAI API key"
            
        input_api_key = st.text_input("OpenAI API Key", 
                                      type="password", 
                                      value=default_api_key,
                                      help=api_key_help)
        
        # è·å–ç°æœ‰é›†åˆ
        collections = get_collections(st.session_state.persist_dir)
        
        # é›†åˆé€‰æ‹©åŒºåŸŸ
        st.subheader("Collection Settings")
        
        # é»˜è®¤é›†åˆåç§°
        default_collection = "documents"
        # å¦‚æœæœ‰ç°æœ‰é›†åˆï¼Œæä¾›é€‰æ‹©
        if collections:
            collection_options = collections
            default_index = 0 if default_collection not in collections else collections.index(default_collection)
            collection_name = st.selectbox(
                "Select Collection",
                options=collection_options,
                index=default_index
            )
            collection_info = get_collection_info(st.session_state.persist_dir, collection_name)
            if collection_info["exists"]:
                st.info(f"Collection contains {collection_info['count']} documents")
        else:
            collection_name = st.text_input("Collection Name", value=default_collection)
            st.info(f"No existing collections found. Please initialize a collection first via CLI.")
        
        # æ¨¡å‹é€‰æ‹©
        st.subheader("Model Settings")
        llm_model = st.selectbox(
            "LLM Model",
            options=[
                "gpt-4o-mini",
                "gpt-3.5-turbo",
                "gpt-4o",
                "o1"
            ],
            index=0
        )
        
        # æ£€ç´¢æ–‡æ¡£æ•°é‡è®¾ç½®
        top_k = st.slider("Number of documents to retrieve", min_value=1, max_value=20, value=5)
        
        # ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„APIå¯†é’¥æˆ–ä¹‹å‰åŠ è½½çš„å¯†é’¥
        used_api_key = input_api_key or api_key
        
        # åˆå§‹åŒ–æŒ‰é’®
        if st.button("Initialize Pipeline"):
            if not used_api_key:
                st.error("Please provide an OpenAI API key.")
            else:
                if initialize_pipeline(used_api_key, llm_model, top_k, collection_name):
                    st.success(f"Pipeline initialized successfully with collection '{collection_name}'!")
                    st.session_state.collection_name = collection_name
    
    # ä¸»åŒºåŸŸ
    if not st.session_state.rag_pipeline:
        st.info("Please initialize the pipeline in the sidebar to get started.")
        
        # æ·»åŠ ä¸€äº›ä½¿ç”¨æŒ‡å—
        with st.expander("Usage Guide", expanded=True):
            st.markdown("""
            ### Getting Started
            1. **Initialize Pipeline**: Provide an OpenAI API key and select a Collection in the sidebar
            2. **Ask Questions**: Use the chat interface below to query your knowledge base
            
            ### About Collections
            - Collections need to be created and loaded with documents using the CLI interface
            - Different topics or projects should be stored in separate Collections
            - Select the appropriate collection for your query context
            """)
            
    else:
        # æ˜¾ç¤ºæ¨¡å‹ä»‹ç»
        if "model_message" in st.session_state:
            st.success(st.session_state.model_message)
            # æ˜¾ç¤ºä¸€æ¬¡åæ¸…é™¤æ¶ˆæ¯
            st.session_state.model_message = None
        
        # æ˜¾ç¤ºå½“å‰é›†åˆçŠ¶æ€
        if "collection_name" in st.session_state:
            collection_info = get_collection_info(st.session_state.persist_dir, st.session_state.collection_name)
            col1, col2, col3 = st.columns(3)
            with col1:
                st.info(f"ğŸ“š Collection: **{st.session_state.collection_name}**")
            with col2:
                if collection_info["exists"]:
                    st.info(f"ğŸ“Š Document Count: **{collection_info['count']}**")
                else:
                    st.warning("âš ï¸ Collection not initialized or does not exist")
            with col3:
                from datetime import datetime
                st.info(f"ğŸ•’ Current Time: **{datetime.now().strftime('%Y-%m-%d %H:%M')}**")
        
        st.divider()
        
        # èŠå¤©ç•Œé¢
        st.subheader("Chat with Your Knowledge Base")
        
        # æ˜¾ç¤ºèŠå¤©å†å²
        for i, (query, answer) in enumerate(st.session_state.chat_history):
            with st.chat_message("user"):
                st.write(query)
            with st.chat_message("assistant"):
                st.write(answer)
        
        # ç”¨æˆ·è¾“å…¥
        user_query = st.chat_input("Ask a question about your documents...")
        
        if user_query:
            with st.chat_message("user"):
                st.write(user_query)
            
            with st.chat_message("assistant"):
                with st.spinner("Generating answer..."):
                    try:
                        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæ˜¾ç¤ºæ¨¡å‹ä»‹ç»
                        intro_text = ""
                        if not st.session_state.chat_history:
                            intro_text = f"_{st.session_state.rag_pipeline.get_model_introduction()}_\n\n"
                            
                        # ç”Ÿæˆå›ç­”
                        answer = st.session_state.rag_pipeline.get_answer(user_query)
                        
                        # æ˜¾ç¤ºå¸¦æœ‰ä»‹ç»çš„ç­”æ¡ˆï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if intro_text:
                            st.markdown(intro_text)
                        st.write(answer)
                        
                        # å°†é—®é¢˜å’Œå›ç­”å­˜å‚¨åœ¨å†å²è®°å½•ä¸­
                        st.session_state.chat_history.append((user_query, answer))
                    except Exception as e:
                        st.error(f"Error generating answer: {e}")
                        import traceback
                        st.error(traceback.format_exc())

if __name__ == "__main__":
    main() 